Computational Robotics: Search and Rescue Final Project
David Elken, Marena Richardson, Michelle Sit

Introduction
The overall goal of this project was to create an autonomous search and rescue robot. The real world application of such a search and rescue bot is to find and identify victims at a disaster site. The technologies necessary to create a robot capable of search and rescue include many of this semester’s topics such as mapping, localization, computer vision, and mobile robotics. For our adapted search and rescue challenge, we were tasked with making our robot autonomously search through a room, identify “victims” to be rescued, and report their location as coordinates on a map. The site of the challenge was the old STAR Center (Campus Center 330), and the victims to be rescued were four rubber balls (red, blue, green, yellow). Our robot uses a combination of search algorithms, SLAM, and computer vision programs to accomplish its goal. 

Implementation
Our neato travels slowly through a room via a structured random walk and uses SLAM to map the room.  As it travels, it uses a combination of colour contouring and the OpenCV Hough Circle Transform to detect the brightly colored balls. When it finds a circle and a large coloured contour of one of the relevant color ranges, it compares how much the color mask and circle detection overlap.  The area with the highest percentage of overlap for each colour is then marked as a ball on our map.

Structured randomized walk
Initially, our robot navigated using a wall following program that led it around the perimeter of the star center. While the navigation worked well, we were not finding the balls or mapping the STAR center very well. Taking a different approach, we shifted to a structured random walk. The robot moved in a straight line until it detected that it was about to run into an object. It then randomly chose a new direction from a list of all the directions that have at least 2 meters of free space. The robot turned to face this new direction and continued. Given sufficient time, the random walk produces good results with both mapping and ball detection.
	 	 	 	
Localization and Mapping
In order to perform both localization and mapping we used hector SLAM mapping, a method of detection we covered earlier in the semester. Essentially, the hector SLAM package uses the neato's lidar to build a map of its environment as it goes. It also takes care of localization, allowing us to know the robot’s location in the map.

Computer Vision
	In earlier projects, members of our team had used color contouring to detect distinctively coloured objects and others had used machine learning and OpenCV libraries as methods of object detection.  We decided to implement both and integrate the two to make our computer vision component more robust.
Colour contouring is typically conducted by specifying a color range and scanning through a given image to determine if there are pixels within that color range.  The detected pixels can then be extracted to a black and white mask or manipulated by other means.  In a black and white mask, the detected pixels are shown as white pixels while pixels outside of the color range are shown in black.  We decided to use HSV values to analyze our images instead of RGB values for more robust color detection.  HSV values take hue, saturation, and brightness into consideration which can be helpful because hue is likely to remain constant even as shadows, environmental lighting, camera filter, and other sources of variability in identifying colors influence how colors are detected by the neato.  To implement this, we used OpenCV’s libraries to limit the color ranges and analyze the information.
Hough Circle Transform is an OpenCV library that uses the Hough Circle algorithm to detect if there are circles in an image.  The algorithm essentially detects a line or shape based on the highest probability that those pixels follow the mathematical formula for that shape.  Each pixel is assigned a cell in a two-dimensional array and a preliminary edge detector is run on the image to filter which pixels may be part of the circle.  For these pixels, the algorithm calculates the likelihood that the pixel is on the edge of the circle using (x - a)2 + (y - b)2 - r2 = 0 and increments the pixel’s cell by one if there is sufficient evidence.  At the end, the cells with the highest values are determined to be part of the circle.  For our project, we used this algorithm to detect circular shapes and applied a blur to the video feed before running the algorithm to help with its accuracy.
Both programs worked well separately although our circle detector returned more false negatives and proved to be less reliable than the color detector.  To make the program more robust, we first tried running the color detector and then using the circle detector on the mask.  However, the lack of a circular shape often threw off the circle detector and it returned too many false positives to work.  We then decided to have our overall program run both detectors and calculate the overlap in results.  The areas with the highest overlap were then considered to be balls which worked very well and was included as our final iteration.

Placing Balls on the Map
	The markers placed on the map to mark the ball locations were published with the visualization markers library.  These markers needed to be placed very carefully according to their relative map location.  Using image processing we calculated the distance to the ball by measuring it’s width.  Since objects appear smaller when further away we were able to generate an equation that would result in the distance to the object based on its width.  Horizontal offset was calculated by measuring the distance from the center of an identified circle transform to the center of the screen.
	By combining these values with orientation from hector slams localization, we were able (with enormous difficulty) to pinpoint each ball on the map in the correct location as a marker.  Each marker was correctly colour coded in order to identify the specific ball that it represented.

Results
	On demo day we ran into some problems with the random walk.  Some of the parameters had to be tweaked to encourage the neato to take more conservative risks when it changed directions.  Hector SLAM also crashed frequently throughout the round which created an inaccurate map and borked our code which depended on orientation data from hector SLAM.  However, the information collected about the location of the balls in the room remained accurate and we were able to map two balls on a map during one of the rounds.  We later tested a wall following program that we could have used during the competition instead of the random walk and it was able to successfully travel around the room without crashing into objects.  In future rounds, we may consider using the wall following program as our search algorithm until we are able to create a more robust structured random walk.  Overall, the program would have worked well if we had been able to travel through the room and none of our programs crashed periodically.

Reflection
What if any challenges did you face along the way?
	The greatest challenge in this project was mapping the balls to map locations.  This was partly due to working with multiple reference frames (the robot’s, the maps, and hector slams). The initial ball mapping program would successfully locate balls and map balls with negligible error, but only when the robot was at the origin. Once it moved the ball was consistently positioned the correct distance from the neato, but invariably at the incorrect angle.  This particular problem took many many many many hours to resolve.  Many hours. Like all the hours.  So many hours.

What would you do to improve your project if you had more time?
	Given more time we would fix navigation.  Our random walk had a tendency to get run the neato into objects.  We had a wall follow that we elected not to use out of concerns that it would not get a good view of the entire room.  It may have been more productive to update this wall follow code to better scan the room.  We would also have the opportunity to find out why hector SLAM continued to fail the day of the competition 

Did you learn any interesting lessons for future robotic programming projects?  These could relate to working on robotics projects in teams, working on more open-ended (and longer term) problems, or any other relevant topic.
We were able to create three different programs that achieved localization and mapping, ball detection, and randomized walks successfully.  However, integrating the three components was a much more difficult and time consuming process that our team did not accurately anticipate.  In the end, we were not able to test our program with the three components as much as we would have liked.  As a running and continuous lesson in engineering, we will be careful to leave more time for implementation in the future.  In terms of learning process, we thoroughly enjoyed incorporating the majority of this class’ major concepts and lessons into one project.  Being able to complete this project has allowed us to fully understand how much we have learned over the course of this semester.
